{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvG89gVnUezRWs9hxmNl1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Data_Science/blob/main/Next_Word_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYnVciiKUfNp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM, Embedding, Dense\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "file = open('data/eng.txt', 'r', encoding='utf8')\n",
        "\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = ''\n",
        "for i in lines:\n",
        "    data = ' '.join(lines)\n",
        "\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('\"', '').replace(\"'\", '')\n",
        "\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "len(sequence_data)\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "# index + 1 because index 0 will be reserved for padding\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "sequences = []\n",
        "\n",
        "# 3 words used to predict next word\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print('the length of sequence are: ', len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "print(('Data: ', X[:10]))\n",
        "print(('Response: ', y[:10]))\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "# convert class vector to binary class metrix\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# In[47]:\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "cheackpoint = ModelCheckpoint('next_word_eng.h5', monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[cheackpoint])\n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "model = load_model('next_word_eng.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = np.array(sequence)\n",
        "    preds = np.argmax(model.predict(sequence))\n",
        "    predicted_word = ''\n",
        "    pred_list = []\n",
        "\n",
        "    for key, val in tokenizer.word_index.items():\n",
        "        # All Match\n",
        "        pred_list.append(key)\n",
        "        if val == preds:\n",
        "            # Best Match\n",
        "            predicted_word = key\n",
        "            break\n",
        "\n",
        "    print(predicted_word, pred_list)\n",
        "    return predicted_word, pred_list\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "while(True):\n",
        "    text = input('Enter your line... ')\n",
        "\n",
        "    if text == '0':\n",
        "        print('Execution completed...')\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(' ')\n",
        "            text = text[-3:]\n",
        "            print(text)\n",
        "\n",
        "            predict_next_word(model, tokenizer, text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Error occurred: ', e)\n",
        "            continue"
      ]
    }
  ]
}